<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Performance on Code Mark</title>
    <link>https://hzren.github.io/blog/tags/performance/</link>
    <description>Recent content in Performance on Code Mark</description>
    <generator>Hugo -- gohugo.io</generator>
    <managingEditor>hzren@outlook.com (renhongzhen)</managingEditor>
    <webMaster>hzren@outlook.com (renhongzhen)</webMaster>
    <lastBuildDate>Fri, 15 Jan 2021 15:07:33 +0800</lastBuildDate><atom:link href="https://hzren.github.io/blog/tags/performance/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Linux内存管理</title>
      <link>https://hzren.github.io/blog/blog/2021-01-15-linux%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/</link>
      <pubDate>Fri, 15 Jan 2021 15:07:33 +0800</pubDate>
      <author>hzren@outlook.com (renhongzhen)</author>
      <guid>https://hzren.github.io/blog/blog/2021-01-15-linux%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/</guid>
      <description>摘自 倪朋飞-极客时间-Linux性能优化实战
内存映射 物理内存也称为主存，大多数计算机用的主存都是动态随机访问内存（DRAM）。只有内核才可以直接访问物理内存。
Linux 内核给每个进程都提供了一个独立的虚拟地址空间，并且这个地址空间是连续的。这样，进程就可以很方便地访问内存，更确切地说是访问虚拟内存。
虚拟地址空间的内部又被分为内核空间和用户空间两部分，不同字长（也就是单个 CPU 指令可以处理数据的最大长度）的处理器，地址空间的范围也不同。
通过这里可以看出，32 位系统的内核空间占用 1G，位于最高处，剩下的 3G 是用户空间。而 64 位系统的内核空间和用户空间都是 128T，分别占据整个内存空间的最高和最低处，剩下的中间部分是未定义的。
进程在用户态时，只能访问用户空间内存；只有进入内核态后，才可以访问内核空间内存。虽然每个进程的地址空间都包含了内核空间，但这些内核空间，其实关联的都是相同的物理内存。这样，进程切换到内核态后，就可以很方便地访问内核空间内存。
既然每个进程都有一个这么大的地址空间，那么所有进程的虚拟内存加起来，自然要比实际的物理内存大得多。所以，并不是所有的虚拟内存都会分配物理内存，只有那些实际使用的虚拟内存才分配物理内存，并且分配后的物理内存，是通过内存映射来管理的。
内存映射，其实就是将虚拟内存地址映射到物理内存地址。为了完成内存映射，内核为每个进程都维护了一张页表，记录虚拟地址与物理地址的映射关系，如下图所示：
页表实际上存储在 CPU 的内存管理单元 MMU 中，这样，正常情况下，处理器就可以直接通过硬件，找出要访问的内存。而当进程访问的虚拟地址在页表中查不到时，系统会产生一个缺页异常，进入内核空间分配物理内存、更新进程页表，最后再返回用户空间，恢复进程的运行。TLB（Translation Lookaside Buffer，转译后备缓冲器）会影响 CPU 的内存访问性能。
TLB 其实就是 MMU 中页表的高速缓存。由于进程的虚拟地址空间是独立的，而 TLB 的访问速度又比 MMU 快得多，所以，通过减少进程的上下文切换，减少 TLB 的刷新次数，就可以提高 TLB 缓存的使用率，进而提高 CPU 的内存访问性能。不过要注意，MMU 并不以字节为单位来管理内存，而是规定了一个内存映射的最小单位，也就是页，通常是 4 KB 大小。这样，每一次内存映射，都需要关联 4 KB 或者 4KB整数倍的内存空间。
页的大小只有 4 KB ，导致的另一个问题就是，整个页表会变得非常大。比方说，仅 32 位系统就需要 100 多万个页表项（4GB/4KB），才可以实现整个地址空间的映射。
决页表项过多的问题，Linux 提供了两种机制，也就是多级页表和大页（HugePage）。多级页表就是把内存分成区块来管理，将原来的映射关系改成区块索引和区块内的偏移。由于虚拟内存空间通常只用了很少一部分，那么，多级页表就只保存这些使用中的区块，这样就可以大大地减少页表的项数。
在发现内存紧张时，系统就会通过一系列机制来回收内存，比如下面这三种方式：
回收缓存，比如使用 LRU（Least Recently Used）算法，回收最近使用最少的内存页面； 回收不常访问的内存，把不常用的内存通过交换分区直接写到磁盘中； 杀死进程，内存紧张时系统还会通过 OOM（Out of Memory），直接杀掉占用大量内存的进程。 第二种方式回收不常访问的内存时，会用到交换分区（以下简称 Swap）。Swap其实就是把一块磁盘空间当成内存来用。它可以把进程暂时不用的数据存储到磁盘中（这个过程称为换出），当进程访问这些内存时，再从磁盘读取这些数据到内存中（这个过程称为换入）。所以，你可以发现，Swap 把系统的可用内存变大了。不过要注意，通常只在内存不足时，才会发生 Swap 交换。并且由于磁盘读写的速度远比内存慢，Swap 会导致严重的内存性能问题。</description>
    </item>
    
    <item>
      <title>CPU优化</title>
      <link>https://hzren.github.io/blog/blog/2021-01-15-cpu%E4%BC%98%E5%8C%96/</link>
      <pubDate>Fri, 15 Jan 2021 14:06:46 +0800</pubDate>
      <author>hzren@outlook.com (renhongzhen)</author>
      <guid>https://hzren.github.io/blog/blog/2021-01-15-cpu%E4%BC%98%E5%8C%96/</guid>
      <description>摘自 倪朋飞-极客时间-Linux性能优化实战
应用程序优化 首先，从应用程序的角度来说，降低 CPU 使用率的最好方法当然是，排除所有不必要的工作，只保留最核心的逻辑。比如减少循环的层次、减少递归、减少动态内存分配等等。
除此之外，应用程序的性能优化也包括很多种方法，我在这里列出了最常见的几种，你可以记下来。
从系统的角度来说，优化 CPU 的运行，一方面要充分利用 CPU 缓存的本地性，加速缓存访问；另一方面，就是要控制进程的 CPU 使用情况，减少进程间的相互影响。具体来说，系统层面的 CPU 优化方法也有不少，这里我同样列举了最常见的一些方法，方便你记忆和使用。
编译器优化：很多编译器都会提供优化选项，适当开启它们，在编译阶段你就可以获得编译器的帮助，来提升性能。比如， gcc 就提供了优化选项 -O2，开启后会自动对应用程序的代码进行优化。 算法优化：使用复杂度更低的算法，可以显著加快处理速度。比如，在数据比较大的情况下，可以用 O(nlogn) 的排序算法（如快排、归并排序等），代替 O(n^2) 的排序算法（如冒泡、插入排序等）。 异步处理：使用异步处理，可以避免程序因为等待某个资源而一直阻塞，从而提升程序的并发处理能力。比如，把轮询替换为事件通知，就可以避免轮询耗费 CPU 的问题。 多线程代替多进程：前面讲过，相对于进程的上下文切换，线程的上下文切换并不切换进程地址空间，因此可以降低上下文切换的成本。 善用缓存：经常访问的数据或者计算过程中的步骤，可以放到内存中缓存起来，这样在下次用时就能直接从内存中获取，加快程序的处理速度。 系统优化 从系统的角度来说，优化 CPU 的运行，一方面要充分利用 CPU 缓存的本地性，加速缓存访问；另一方面，就是要控制进程的 CPU 使用情况，减少进程间的相互影响。具体来说，系统层面的 CPU 优化方法也有不少，这里我同样列举了最常见的一些方法，方便你记忆和使用。
具体来说，系统层面的 CPU 优化方法也有不少，这里我同样列举了最常见的一些方法，方便你记忆和使用。
CPU 绑定：把进程绑定到一个或者多个 CPU 上，可以提高 CPU 缓存的命中率，减少跨CPU 调度带来的上下文切换问题。 CPU 独占：跟 CPU 绑定类似，进一步将 CPU 分组，并通过 CPU 亲和性机制为其分配进程。这样，这些 CPU 就由指定的进程独占，换句话说，不允许其他进程再来使用这些CPU。 优先级调整：使用 nice 调整进程的优先级，正值调低优先级，负值调高优先级。优先级的数值含义前面我们提到过，忘了的话及时复习一下。在这里，适当降低非核心应用的优先级，增高核心应用的优先级，可以确保核心应用得到优先处理。为进程设置资源限制：使用 Linux cgroups 来设置进程的 CPU 使用上限，可以防止由于某个应用自身的问题，而耗尽系统资源。 NUMA（Non-Uniform Memory Access）优化：支持 NUMA 的处理器会被划分为多个 node，每个 node 都有自己的本地内存空间。NUMA 优化，其实就是让 CPU 尽可能只访问本地内存。 中断负载均衡：无论是软中断还是硬中断，它们的中断处理程序都可能会耗费大量的CPU。开启 irqbalance 服务或者配置 smp_affinity，就可以把中断处理过程自动负载均衡到多个 CPU 上。 常见的内核线程### kswapd0：用于内存回收。在 Swap 变高 案例中，我曾介绍过它的工作原理。 kworker：用于执行内核工作队列，分为绑定 CPU （名称格式为 kworker/CPU:ID）和未绑定 CPU（名称格式为 kworker/uPOOL:ID）两类。 migration：在负载均衡过程中，把进程迁移到 CPU 上。每个 CPU 都有一个 migration 内核线程。 jbd2/sda1-8：jbd 是 Journaling Block Device 的缩写，用来为文件系统提供日志功能，以保证数据的完整性；名称中的 sda1-8，表示磁盘分区名称和设备号。每个使用了 ext4 文件系统的磁盘分区，都会有一个 jbd2 内核线程。 pdflush：用于将内存中的脏页（被修改过，但还未写入磁盘的文件页）写入磁盘（已经在3.</description>
    </item>
    
    <item>
      <title>中断</title>
      <link>https://hzren.github.io/blog/blog/2021-01-15-%E4%B8%AD%E6%96%AD/</link>
      <pubDate>Fri, 15 Jan 2021 10:54:40 +0800</pubDate>
      <author>hzren@outlook.com (renhongzhen)</author>
      <guid>https://hzren.github.io/blog/blog/2021-01-15-%E4%B8%AD%E6%96%AD/</guid>
      <description>摘自 倪朋飞-极客时间-Linux性能优化实战
中断是一种异步的事件处理机制，可以提高系统的并发处理能力。
由于中断处理程序会打断其他进程的运行，所以，为了减少对正常进程运行调度的影响，中断处理程序就需要尽可能快地运行。如果中断本身要做的事情不多，那么处理起来也不 会有太大问题；但如果中断要处理的事情很多，中断服务程序就有可能要运行很长时间。
特别是，中断处理程序在响应中断时，还会临时关闭中断。这就会导致上一次中断处理完成之前，其他中断都不能响应，也就是说中断有可能会丢失。
软中断 为了解决中断处理程序执行过长和中断丢失的问题，Linux 将中断处理过程分成了两个阶段，也就是上半部和下半部：
上半部用来快速处理中断，它在中断禁止模式下运行，主要处理跟硬件紧密相关的或时间敏感的工作。 下半部用来延迟处理上半部未完成的工作，通常以内核线程的方式运行。 以最常见的网卡接收数据包为例：
网卡接收到数据包后，会通过硬件中断的方式，通知内核有新的数据到了。这时，内核就应该调用中断处理程序来响应它。你可以自己先想一下，这种情况下的上半部和下半部分别负责什么工作呢？
对上半部来说，既然是快速处理，其实就是要把网卡的数据读到内存中，然后更新一下硬件寄存器的状态（表示数据已经读好了），最后再发送一个软中断信号，通知下半部做进一步的处理。 而下半部被软中断信号唤醒后，需要从内存中找到网络数据，再按照网络协议栈，对数据进行逐层解析和处理，直到把它送给应用程序。 所以，这两个阶段你也可以这样理解：
上半部直接处理硬件请求，也就是我们常说的硬中断，特点是快速执行； 而下半部则是由内核触发，也就是我们常说的软中断，特点是延迟执行。 实际上，上半部会打断 CPU 正在执行的任务，然后立即执行中断处理程序。而下半部以内核线程的方式执行，并且每个 CPU 都对应一个软中断内核线程，名字为 “ksoftirqd/CPU编号”，比如说， 0 号 CPU 对应的软中断内核线程的名字就是 ksoftirqd/0。
不过要注意的是，软中断不只包括了刚刚所讲的硬件设备中断处理程序的下半部，一些内核自定义的事件也属于软中断，比如内核调度和 RCU 锁（Read-Copy Update 的缩写，RCU 是 Linux 内核中最常用的锁之一）等。
查看软中断和内核线程 不知道你还记不记得，前面提到过的 proc 文件系统。它是一种内核空间和用户空间进行通信的机制，可以用来查看内核的数据结构，或者用来动态修改内核的配置。其中：
/proc/softirqs 提供了软中断的运行情况 /proc/interrupts 提供了硬中断的运行情况 运行下面的命令，查看 /proc/softirqs 文件的内容，你就可以看到各种类型软中断在不同CPU 上的累积运行次数：
[root@testswarm1 ~]# cat /proc/softirqs CPU0 CPU1 CPU2 CPU3 HI: 0 0 0 0 TIMER: 12631619 232062373 248462702 893005335 NET_TX: 8896509 4597175 4634277 4587284 NET_RX: 2318866690 2162127770 2156810551 2166304170 BLOCK: 122868813 128777565 140769264 127375707 IRQ_POLL: 0 0 0 0 TASKLET: 4994443 830265 910880 1602220 SCHED: 1556077017 1449705571 1371002788 1403110449 HRTIMER: 14076 12295 11558 14612 RCU: 3175872103 3298281937 3300542031 38936799 在查看 /proc/softirqs 文件内容时，你要特别注意以下这两点。</description>
    </item>
    
    <item>
      <title>DMA 简介</title>
      <link>https://hzren.github.io/blog/blog/2020-12-31-direct-memory-access-dma-%E7%AE%80%E4%BB%8B/</link>
      <pubDate>Thu, 31 Dec 2020 10:45:29 +0800</pubDate>
      <author>hzren@outlook.com (renhongzhen)</author>
      <guid>https://hzren.github.io/blog/blog/2020-12-31-direct-memory-access-dma-%E7%AE%80%E4%BB%8B/</guid>
      <description>摘自 倪朋飞-极客时间-Linux性能优化实战
概述 编程式I/O
在编程I/O中，处理器不断扫描是否有任何设备准备好进行数据传输。如果一个I/O设备准备好了，处理器就会全力以赴地在I/O和内存之间传输数据。它以高速率传输数据，但在数据传输过程中不能参与任何其他活动。这是编程I/O的主要缺点。
中断式 I/O
在中断式 I/O 中，每当设备准备好进行数据传输时，就会向处理器发出一个中断。处理器完成正在执行的指令并保存其当前状态。然后它切换到数据传输，这会导致延迟。在这里，处理器不会一直扫描准备进行数据传输的外围设备。但是，它完全参与了数据传输过程。因此，这也不是一种有效的数据传输方式。
DMA
与编程式 I/O 和中断式 I/O 不同，Direct Memory Access 是一种在不通过 CPU 就可以在内存和外部设备之间（外部设备和外部设备之间也是）传输数据的技术。DMA通过DMA控制器实现。
DMA通过接管 CPU 传输数据的任务来提高 CPU 的使用效率和 I/O 的传输速度，在接管期间，CPU 可以空闲出来去做别的任务。该技术克服了其它两种 I/O 技术在发出数据传输命令时耗时、数据传输时占用处理器而导致数据处理功能被浪费的缺点。
当需要传输大量数据时，使用DMA方法更有效。为了实现DMA，处理器必须与DMA模块共享其系统总线。因此，DMA模块只能在处理器不需要总线时才能使用总线，否则必须强制处理器暂时挂起，让出系统铜线。在实际当中，后一种技术更为常用，称为 cycle stealing.
下图展示了指令周期中的附加DMA模块周期:
DMA 基本操作 当处理器希望读取或发送数据块时，它通过向DMA模块发送一些信息来向DMA模块发出命令。这些信息包括：
读或写命令，通过读写控制线发送 要读取或写入的字数，在数据线上进行通信并存储在数据计数寄存器中 在存储器中读写的起始位置，通过数据线进行通信并存储在地址寄存器中 所涉及的I/O设备的地址，通过数据线进行通信 在信息发送后，处理器將继续进行其他工作。DMA模块随后将整个数据块直接传输至内存或直接从内存中传输出来，而无需经过处理器。在传输完成时，DMA模块会向处理器发送一个中断信号，通知它 DMA 已完成使用系统总线。
DMA 基本配置 DMA 可以通过以下几种方式进行实现，包括：
单总线，分离式 DMA 单总线，集成式 DMA-I/O I/O 总线 模式 单总线，分离式 DMA 所有模块共享同一个系统总线。DMA模块充当代理处理器，它使用编程式 I/O 的方式在内存和 I/O 设备之间通过 DMA 模块进行交换数据。这种配置虽然便宜，但效率很低。这是因为一个字的每次传输都需要消耗两个总线周期。
单总线，集成式 DMA-I/O 在此模式中，DMA 模块和一个或多个不包括系统总线在内的 I/O 模块之间存在一条路径。DMA逻辑可以是 I/O 模块的一部分，也可以是控制一个或多个I/O模块的单独模块。因此，所需的总线周期的数量可以大幅度减少。DMA 模块与处理器和内存共享的系统总线仅用于与内存交换数据。DMA和I/O模块之间的数据交换通过系统总线进行。</description>
    </item>
    
  </channel>
</rss>
